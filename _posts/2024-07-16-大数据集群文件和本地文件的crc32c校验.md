---
title: 大数据集群文件和本地文件的CRC32C校验
date: 2024-07-16 17:44 +0800
categories: [大数据, HDFS]
tags: [hdfs]
---

## 背景
本地数据上传至集群时，虽然做了拆分块的 checksum，但是没有做整体的 checksum，因此上传数据后，有强需求的话，可以做下集群文件和本地文件的整体 checksum 验证。

## 问题
HDFS中有两个配置和 checksum 相关

`dfs.checksum.type`  默认为 CRC32C，是 hdfs 传输文件时的默认 checksum。

`dfs.checksum.combine.mode`  定义块级别的 checksum 如何整合成文件级别的 checksum，默认是`MD5MD5CRC`，每个块的校验段做 CRC32C 校验，然后总和做 MD5，最后所有数据块的 MD5 校验值再进行一次 MD5 计算，得到整个文件的 MD5 校验值。

- 坑点 1：这里使用默认的`MD5MD5CRC`，拿到的其实不是文件本身数据整体的 CRC checksum，而是多层级联后的 checksum，跟本地的 checksum 肯定不同。
- 坑点 2：MD5MD5CRC跟块大小和块分布模式（比如 EC 纠删码存储的块模式）相关，不同存储模式的文件不能直接比对。
- 坑点 3：本地进行 CRC 校验的时候一般用 perl 的 CRC32，但是集群用的 CRC32C，算法不一样，哈希会有差异。

## 解法
### 解决集群文件级CRC校验配置
调整`dfs.checksum.combine.mode`的 checksum 配置，可以直接在 checksum 命令中通过`-D`来覆盖默认配置。可以使用`COMPOSITE_CRC`配置，即整个文件级别的 CRC 校验。参考命令 `hdfs dfs -Ddfs.checksum.combine.mode=COMPOSITE_CRC -checksum 文件路径`

### 补充Linux的CRC32C校验方式
```bash
# 先创建一个python 的 venv 虚拟环境，和现有本机的 python 环境隔离
python3 -m venv /data/mtools/.env/crccheck
# 激活虚拟环境
python3 -m venv /data/mtools/.env/crccheck/bin/activate
# 升级下 pip 版本，需要最新版本
pip install --upgrade pip
# 安装 gsutil，我们用 gsutil 提供的哈希能力来拿到 crc32c 的校验
pip install gsutil
```

安装完毕后，确认下 gsutil 工作正常，就可以使用了，案例如下：
```bash
# 激活虚拟环境
source /data/mtools/.env/crccheck/bin/activate

# 验证HDFS上文件checksum
hdfs dfs -Ddfs.checksum.combine.mode=COMPOSITE_CRC -checksum /xxxxxxx/part-0
# 这里返回是 文件名+CRC规格+hex输出的结果，比如 /xxxxxxx/part-0	COMPOSITE-CRC32C	dc4ead94

# 验证本地文件的checksum
gsutil hash -c -h part-0 | grep crc32c | awk '{print $3}'
# 这里输出是大写的 hex 输出的哈希，这里-c 的 flag 代表crc32c checksum，-h 代表 hex 输出，默认是 base64 的，要注意。 最终结果是：DC4EAD94
```