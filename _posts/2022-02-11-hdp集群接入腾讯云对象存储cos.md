---
title: HDP集群接入腾讯云对象存储COS
date: 2022-02-11 14:50 +0800
categories: [大数据, HDFS]
tags: [hdfs, cos，hdp]
---

## 参考资料
别的不说，先上参考资料为敬

[腾讯云对象存储文档](https://cloud.tencent.com/document/product/436/6884)

[腾讯云hadoop-cos git仓库](https://github.com/tencentyun/hadoop-cos)

## 背景及总结
在金融网络系统的高等级保障要求中，一般要求关键数据做异地冷备。如果为此建设大数据备份集群，有成本偏高，双集群运维，同步流程复杂等问题。

在调研腾讯云EMR方案中，云EMR对COS的使用，给了我启发，如果将COS直接用于现有深圳集群的冷备存储，可以减少很大一部分使用及维护成本。数据冷备作为一次写入，极少读取的场景，选择对象存储承载，十分适合。冷备数据利用腾讯云的COS低频存储能力，能更进一步的压缩存储成本。

这里对HDP集群配置COS接入的流程做下梳理总结。

## 部署方式
### 所需jar包及配置位置
接入腾讯云COS，需要使用云提供的hadoop-cos插件，在hadoop-3.3.0及以后的版本中，已经默认包含了相应包，HDP集群是3.1.0版本，因此，需要使用云侧编译好的hadoop-cos插件进行补充处理。
这里历史版本的hadoop-cos是有问题的，需要注意（hadoop-cos依赖的cos-sdk默认将连接方式配置为了https，而历史版本中hadoop-cos针对http protocal的配置不生效），需要选择pre-release的最新版本hadoop-cos，从Git上下载最新的hadoop-cos-3.1.0-8.0.3.jar以及依赖的cos_api-bundle-5.6.65.jar包，并将jar包放入所有机器的对应路径下（建议脚本处理，创建distribute_hadoop_cos_jars.sh）
以3.1.0版本的HDP集群为例，需要配置jar的路径如下：
``` 
/usr/hdp/3.1.0.0-78/hadoop-hdfs/lib/
/usr/hdp/3.1.0.0-78/hadoop/lib/
/usr/hdp/3.1.0.0-78/hadoop-mapreduce/lib/
/usr/hdp/3.1.0.0-78/hadoop-yarn/lib/
/usr/hdp/3.1.0.0-78/hive/lib/
``` 

**需要将jar包的权限设为644，否则会有读取权限问题。**

### HDFS配置项
在hdfs的custom core-site内配置以下参数：
```yaml
//使用腾讯云上API密钥的ID
fs.cosn.userinfo.secretId=xxxx
//使用腾讯云上API密钥的KEY
fs.cosn.userinfo.secretKey=xxxx
//这里配置cos桶所在的网区，需要与HDP集群所在的区域一致，否则会从公网访问，产生流量及带宽费用
fs.cosn.bucket.region=ap-guangzhou
fs.cosn.useHttps=false
fs.cosn.impl=org.apache.hadoop.fs.CosFileSystem
fs.AbstractFileSystem.cosn.impl=org.apache.hadoop.fs.CosN
//临时文件存放路径
fs.cosn.tmp.dir=/tmp/hadoop_cos
fs.cosn.maxRetries=3
fs.cosn.retry.interval.seconds=3
//配置开启crc32 checksum能力，方便使用hdfs dfs -checksum来做一致性校验
fs.cosn.crc32c.checksum.enabled=true
```

配置完毕后，需要重启要求重启的组件。

### Mapreduce/Tez配置项
因为HDP下Mapreduce/Tez任务默认不从/usr/hdp/{version}下获取lib，而是每次从tar包解压缩出lib jars，因此需要引入外部jar包
在mapreduce组件的```mapreduce.application.classpath```配置项末尾添加```/etc/hadoop/conf/secure:/usr/hdp/${hdp.version}/hadoop-mapreduce/lib/cos_api-bundle-5.6.65.jar:/usr/hdp/${hdp.version}/hadoop-mapreduce/lib/hadoop-cos-3.1.0-8.0.3.jar```配置。
同理，Tez组件中的```tez.cluster.additional.classpath.prefix```配置项也需要这样配置。

## 功能验证

### 读取COS数据
参照上述流程配置完成hadoop-cos插件后直接使用hdfs提供的文件系统操作命令即可完成各类操作
``` 
//列出所有路径文件
hdfs dfs -ls cosn://xxxxxx-xxxxxx/
//结果
22/02/09 17:10:49 INFO fs.CosNativeFileSystemStore: hadoop cos retry times: 3, cos client retry times: 5
22/02/09 17:10:49 INFO cosn.BufferPool: fs.cosn.upload.buffer.size is set to -1, so the 'mapped_disk' buffer will be used by default.
22/02/09 17:10:49 INFO cosn.BufferPool: The type of the upload buffer pool is [MAPPED_DISK]. Buffer size:[-1]
22/02/09 17:10:49 INFO fs.CosFileSystem: cos file system bucket is merged false
Found 3 items
drwxrwxrwx   - root root          0 1970-01-01 08:00 cosn://xxxxxx-xxxxxx/hive-test-data
drwxrwxrwx   - root root          0 1970-01-01 08:00 cosn://xxxxxx-xxxxxx/user
-rw-rw-rw-   1 root root      13328 2022-01-26 16:32 cosn://xxxxxx-xxxxxx/初始化cos权限数据.sql
22/02/09 17:10:49 INFO fs.CosFileSystem: begin to close cos file system
22/02/09 17:10:49 INFO cosn.BufferPool: Close a buffer pool instance.
22/02/09 17:10:49 INFO cosn.BufferPool: Begin to release the buffers.
```

### 通过hdfs dfs操作将数据导入COS
直接使用cp拷贝的方案，是单线程一个一个文件传输的（单文件会拆成多个数据块，利用多个http连接传输）
实验拷贝3.2G左右数据 77个文件
hdfs dfs -cp -f /tmp/export-to-hd/xxxxxx.t_dwd_all_gzh_user_analysis_dd cosn://xxxxxx-xxxxxx/hive-test-data
总耗时约3分钟左右

### 使用Mapreduce通过distcp导入
通过distcp导入有两种方案，一是使用hadoop自身的distcp能力导入，二是使用腾讯云提供的cos-distcp工具进行数据导入。
#### 方案一 hadoop distcp
测试命令如下：
``` 
hadoop distcp -Dmapreduce.job.name=test_table_migration -m 20 -update -delete -skipcrccheck -i /warehouse/tablespace/managed/hive/xxxxxx.db/test_table_cos cosn://xxxxxx-xxxxxx/hive-test-data/warehouse/xxxxxx.db/test_table_cos
```
验证启动任务，拷贝总耗时约1分钟。当数据量更大的时候，预计和“通过hdfs dfs操作将数据导入COS”的方案时间差异会进一步拉大。

注意，这里必须使用-skipcrccheck来取消distcp本身的checksum流程，因为hadoop distcp工具使用的crc32checksum和COS这边应用的crc32checksum方法不兼容。

使用hadoop的distcp，需要配置好mapreduce的外部jar，否则会有```Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.CosFileSystem not found```问题。

#### 方案二 cos distcp
从腾讯云下载 cos-distcp-1.9-3.1.0.jar 包，上传至hdfs或集群节点上。
工具文档地址：https://cloud.tencent.com/document/product/436/50272
测试命令如下：
``` 
hadoop jar cos-distcp-1.9-3.1.0.jar --src=/warehouse/tablespace/managed/hive/xxxxxx.db/test_table_cos --dest=cosn://xxxxxx-xxxxxx/hive-test-data/warehouse/xxxxxx.db/test_table_cos --taskNumber=10 --workerNumber=4
```
验证启动任务，拷贝总耗时约1分钟，cos distcp工具也做了crc校验，prometheus监控等功能，比hadoop原生工具更加适合生产使用。
大文件会有同步失败的情况，mapreduce的container在copy 的时候出现 Status Code: 502错误，会卡住，然后一直重复执行。需要调大-Dfs.cosn.upload.part.size的值。

### 验证COS作为Hive表单的存储路径
使用Alter命令变更hive表单的location元信息：
``` 
ALTER TABLE xxxxxx.test_table_cos SET LOCATION "cosn://xxxxxx-xxxxxx/hive-test-data/warehouse/xxxxxx.db/test_table_cos/";
```
检查读取表单数据正常。

### 对COS和HDFS中的Hive表行数校验测试
HDFS存储内的源表：```xxxxxx.test_table_source```
COS存储内的表：```xxxxxx.test_table_cos```
测试sql
``` 
select count(*) from xxxxxx.t_dwd_all_gzh_user_analysis_dd;
select count(*) from xxxxxx.test_table_cos;
```
计算HDFS源表行数时，耗时24秒。
计算COS表行数时，耗时557秒，差异十分大，这里主要卡在tez session创建阶段。第二次查询耗时19秒。
两次查询都启动了Tez任务，估计是集群规格配置问题的可能性比较大，本次用的集群性能比较差，生产上应该不会有类似问题。

### 删除逻辑测试
直接使用删除命令即可
``` 
hdfs dfs -rm -r -skipTrash cosn://xxxxxxx/hive-test-data/warehouse/xxxxxx.db/xxxxxx
22/02/09 15:17:04 INFO fs.CosNativeFileSystemStore: hadoop cos retry times: 3, cos client retry times: 5
22/02/09 15:17:05 INFO cosn.BufferPool: fs.cosn.upload.buffer.size is set to -1, so the 'mapped_disk' buffer will be used by default.
22/02/09 15:17:05 INFO cosn.BufferPool: The type of the upload buffer pool is [MAPPED_DISK]. Buffer size:[-1]
22/02/09 15:17:05 INFO fs.CosFileSystem: cos file system bucket is merged false
Deleted cosn://xxxxxxx/hive-test-data/warehouse/xxxxxx.db/xxxxxx
22/02/09 15:17:06 INFO fs.CosFileSystem: begin to close cos file system
22/02/09 15:17:06 INFO cosn.BufferPool: Close a buffer pool instance.
22/02/09 15:17:06 INFO cosn.BufferPool: Begin to release the buffers.
```
注意，删除数据时如果没加skipTrash参数，清理掉的数据会留存在cos内root用户的trash路径，应该是直接修改了路径位置。可以使用`-skipTrash` flag跳过回收站。

## 透过网关传输COS请求时遇到的问题
使用网关方案连接COS的话，会有报错问题，请求上传分块成功后，回传返回的 ETag 响应头部的值不能被组件正常获取，打印etag的时候，是空值。
问题原因：网关使用基于GO语言的http公共库转发请求，Go的http库的各类http头处理方法，会默认将各类请求头改写成驼峰命名的形式（HTTP标准形式），而Java版本COS SDK中，存在多种非标准的HTTP Header情况，且从头部获取metadata时，相关代码虽然在验证是否存在头部时忽略了大小写，但是在取值时仍使用非标的头部key获取，导致获取出来的结果为空。
解决方法：目前网关侧针对cos的特殊请求头部做了处理，在头部打包后重新编辑key，改写成COS的SDK能够正常获取的形式。

## 数据冷备的流程
数据冷备采用惯例的任务管理器（manager） + 多执行器（handler）的架构，以便灵活的控制并发度，控制任务的生成策略，控制冷备任务的启动策略，留存备份任务的状态，执行结果，校验结果等。另外还有独立的数据校验流程，周期性运行，使用CRC32 Checksum来保证冷备历史数据的一致性，及时发现历史数据重写和数据变更情况。

任务管理器逻辑包括：
1. 根据规则，从元数据提取最新关键数据表单列表
2. 依据表单，生成task列表

其中，符合冷备需求的表单规则可以有多种匹配方式：
	1. 根据表单名进行精确匹配
	2. 以整库形式进行匹配，并可配置关键词黑名单

执行器逻辑包括：
1. 执行器进程遍历查询task列表，寻找未被处理的任务
2. 判断任务相关表单是否正在写入过程中（检查是否存在hive staging路径）
3. 校验分区数，数据量级，数据量小的直接export整表到COS目标路径中（export逻辑会自动跳过文件名/大小相同的文件，因此这部分会完成历史分区的校验比对）（可能需要排掉大数据量下distcp的情况，这个触发的机制要明确）。数据量大，分区数多的，为了防止export逻辑检验耗时过长，则采用提取新增分区进行同步的方式。
4. 根据策略，比对元数据中多出来的分区，删除冗余分区（支持冗余分区保留的形式）
5. 在独立库中import export出的数据成外部表
6. 进行表单的分区数据校验，校验行数，结果入库
7. 如果上述逻辑存在异常，重试三次

历史分区数据的周期性校验逻辑：
1. 从已完成的task中获取同步的表单信息，进行元数据比对
2. 提取细化的元数据信息，遍历最小分区
3. 进入最小分区路径中，随机选取文件，取文件名
4. 直接针对该文件名进行COS中的文件，及hdfs中的文件的CRC32校验，结果不一致告警，人工处理。