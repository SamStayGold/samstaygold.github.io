---
title: kyuubi小文件合并能力测试
date: 2024-07-19 14:56 +0800
categories: [大数据, Kyuubi]
tags: [kyuubi]
---


## Spark 小文件问题
Hive 表中太多的小文件会影响数据的查询性能和效率，同时加大了 HDFS NameNode 的压力。Hive (on MapReduce) 一般可以简单的通过一些参数来控制小文件，而 Spark 中并没有提供小文件合并的功能。下面我们来简单了解一下 Spark 小文件问题，以及如何处理小文件。

## 环境
Kyuubi 版本：1.6.0-SNAPSHOT

Spark 版本：3.1.3、3.2.0

TPCDS 数据集
Kyuubi 中提供了一个 TPCDS Spark Connector，可以通过配置 Catalog 的方式，在读取时自动生成 TPCDS 数据。

只需要将 kyuubi-spark-connector-tpcds_2.12-1.6.0-SNAPSHOT.jar 包放入 Spark jars 目录中，并配置：

`spark.sql.catalog.tpcds=org.apache.kyuubi.spark.connector.tpcds.TPCDSCatalog;`
这样我们就可以直接读取 TPCDS 数据集：

```sql
use tpcds;
show databases;
use sf3000;
show tables;
select * from sf300.catalog_returns limit 10;
```

## 小文件产生
首先我们在 Hive 中创建一个 sample.catalog_returns 表，用于写入生成的 TPCDS catalog_returns 数据，并添加一个 hash 字段作为分区。
```sql
create table tmp_db.catalog_returns
(
    cr_returned_date_sk      bigint,
    cr_returned_time_sk      bigint,
    cr_item_sk               bigint,
    cr_refunded_customer_sk  bigint,
    cr_refunded_cdemo_sk     bigint,
    cr_refunded_hdemo_sk     bigint,
    cr_refunded_addr_sk      bigint,
    cr_returning_customer_sk bigint,
    cr_returning_cdemo_sk    bigint,
    cr_returning_hdemo_sk    bigint,
    cr_returning_addr_sk     bigint,
    cr_call_center_sk        bigint,
    cr_catalog_page_sk       bigint,
    cr_ship_mode_sk          bigint,
    cr_warehouse_sk          bigint,
    cr_reason_sk             bigint,
    cr_order_number          bigint,
    cr_return_quantity       int,
    cr_return_amount         decimal(7, 2),
    cr_return_tax            decimal(7, 2),
    cr_return_amt_inc_tax    decimal(7, 2),
    cr_fee                   decimal(7, 2),
    cr_return_ship_cost      decimal(7, 2),
    cr_refunded_cash         decimal(7, 2),
    cr_reversed_charge       decimal(7, 2),
    cr_store_credit          decimal(7, 2),
    cr_net_loss              decimal(7, 2)
) PARTITIONED BY(hash int);
```
我们先关闭 Kyuubi 的优化，读取 catalog_returns 数据并写入 Hive：

set spark.sql.optimizer.insertRepartitionBeforeWrite.enabled=false;

insert overwrite tmp_db.catalog_returns partition (hash=0) select * from tpcds.sf300.catalog_returns;
Spark SQL 最终产生的文件数最多可能是最后一个写入的 Stage 的 Task 数乘以动态分区的数量。我们可以看到由于读取输入表的 Task 数是 44 个，所以最终产生了 44 个文件，每个文件大小约 69 M。

